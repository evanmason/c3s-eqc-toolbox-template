

# https://docs.dask.org/en/latest/array-best-practices.html
#export OMP_NUM_THREADS=1
#export MKL_NUM_THREADS=1
# export OPENBLAS_NUM_THREADS=1

import warnings
from glob import glob
from numba import jit, njit, prange
import dask
import netCDF4
import xarray as xr
from math import sqrt
#from xarray import ufuncs 
from numpy.polynomial import Polynomial
from numpy import (
    linspace,
    empty,
    isfinite,
    where,
    isnan,
    cos,
    zeros_like,
    hstack,
    empty_like,
    full,
    nan,
    zeros,
    quantile,
    array,
    deg2rad,
    ma,
    log1p,
    exp,
    ones_like,
    #sqrt,
    nanmean,
    nanstd,
    nanmin,
    nanmax,
    interp,
    mean,
    std,
    ufunc,
    polyval,
    polynomial,
    polyfit,
    poly1d,
    int32,
    int64,
    float32,
    float64,
    arange,
)

@njit(cache=True, parallel=True)
def get_uvspd_quantile(u, v, out):
    """
    # Extrema:
    # Extract the UVspd values for those time steps for which U, V exceed
    # 95th percentile and then average these values of U, V.
    """
    out[:] = nan
    uv = numpy.sqrt(u ** 2 + v ** 2)
    for j in prange(uv.shape[1]):
        for i in prange(uv.shape[2]):
            idx = (~isnan(uv[:, j, i])).nonzero()[0]
            if idx.size > 0:
                out[0, j, i] = quantile(uv[idx, j, i], 0.95)
                uv_k = (uv[idx, j, i] > out[0, j, i]).nonzero()[0]  # indices to 95th
                out[0, j, i] = (uv[idx[uv_k], j, i]).mean()  # 95th
    return out


def get_slope(ugeo, vgeo, time):
    """
    # Trend
    """
    time, nyear = time[0], time[1]
    out = full((2, *ugeo[0].shape), nan, dtype=float64)
    ffit = full((2, *ugeo.shape), nan, dtype=float64)
    for j in range(ugeo.shape[1]):
        for i in range(ugeo.shape[2]):

            idx = (~isnan(time) & (~isnan(ugeo[:, j, i]))).nonzero()[0]
            idy = (~isnan(time) & (~isnan(vgeo[:, j, i]))).nonzero()[0]

            if idx.size > 0:
                #pcoeff = polynomial.polynomial.Polynomial.fit(time[idx], ugeo[idx, j, i], deg=1)
                pcoeff = polyfit(time[idx], ugeo[idx, j, i], deg=1)[0]
                print("TTTTTTTTT", pcoeff)
                ffit[0, :, j, i] = polynomial.polynomial.polyval(time[idx], pcoeff)
                #ffit[0, :, j, i] = fit(time[idx], ugeo[idx, j, i], deg=1)
                out[0, j, i] = ffit[0, -1, j, i]
                out[0, j, i] -= ffit[0, 0, j, i]

            if idy.size > 0:
                #pcoeff = polynomial.polynomial.Polynomial.fit(time[idy], vgeo[idy, j, i], deg=1)
                pcoeff = polyfit(time[idy], vgeo[idy, j, i], deg=1)[0]
                ffit[1, :, j, i] = polynomial.polynomial.polyval(time[idy], pcoeff)
                #ffit[0, :, j, i] = fit(time[idy], vgeo[idy, j, i], deg=1) 
                #ffit[1, :, j, i] = polynomial.polynomial.polyval(time, pcoeff)#.coef)
                out[1, j, i] = ffit[1, -1, j, i]
                out[1, j, i] -= ffit[1, 0, j, i]

            out[:, j, i] *= 1000
            out[:, j, i] /= nyear  # trend in mm s-1 year-1
    return out

def str_compare(x, y):
    return True if x in y and y in x else False



def str_compare(x, y):
    return True if x in y and y in x else False




if __name__ == "__main__":

    # data_sets = [
    # "Altimetry",
    # "ORAS5",
    # ]

    data_set = "Altimetry"
    #data_set = "ORAS5"
    # data_set = "BRAN2020"
    # data_set = 'Mercator'
    # data_set = "BlackSea"
    #data_set = "MedSea"


    print(data_set)



    free_period = True

    notify = False
    # savedir = '/path/to/save/'
    #savedir = "/home/emason/Dropbox/C3S/code_C3S/Evan_C3S_figures/"
    #savedir = "/home/emason/Dropbox/C3S/code_C3S/Evan_C3S_BlackSea_figures/"
    #savedir = "/marula/emason/"
    savedir = "/home/emason/"


    if str_compare(data_set, "Altimetry"):
        # directory = "/marula/emason/data/Copernicus/CDS/"
        #directory = "/home/emason/code_C3S/vDT2018/"
        directory = "/home/emason/Dropbox/C3S/code_C3S/"
        #directory = "/marula/emason/C3S2_520/Global_Ocean/"
        directory = "/data/emason/"
        drop_variables = [
            #"ugosa",
            #"vgosa",
            #"adt",
            "err",
            "err_sla",
            "err_ugosa",
            "err_vgosa",
            #"sla",
            "crs",
            "lat_bnds",
            "lon_bnds",
            "tpa_correction",
            "flag_ice",
        ]
        variables = dict(
            [
                ("time", "time"),
                ("lon", "longitude"),
                ("lat", "latitude"),
                ("U", "ugos"),
                ("V", "vgos"),
            ]
        )


        if free_period:
            ncfiles = sorted(
                glob(directory + "dt_global_twosat_phy_l4_200[0-6]????_vDT2021.nc")
            )
        else:
            ncfiles = glob(
                directory + "dt_global_twosat_phy_l4_200[7-9]????_vDT2021.nc"
                #dt_global_twosat_phy_l4_20070614_vDT2021.nc
            )
            ncfiles += glob(
                directory + "dt_global_twosat_phy_l4_201[0-8]????_vDT2021.nc"
            )
            ncfiles = sorted(ncfiles)

        chunks = {
            variables["time"]: "365",
            #variables["lat"]: 360,
            #variables["lon"]: 720,
        }

    # BlackSea
    elif str_compare(data_set, "BlackSea"):
        directory = "/marula/emason/C3S2_520/Black_Sea/"
        drop_variables = [
            "crs",
            "lat_bnds",
            "lon_bnds",
            "nv",
            "err",
        ]
        variables = dict(
            [
                ("time", "time"),
                ("lat", "latitude"),
                ("lon", "longitude"),
                ("sla", "sla"),
                ("U", "ugosa"),
                ("V", "vgosa"),
            ]
        )
        if free_period:
            ncfiles = sorted(
                glob(directory + "dt_blacksea_twosat_phy_l4_????????_vDT2018.nc")
            )
            chunks = {
                variables["time"]: "auto",
                variables["lat"]: 100,
                variables["lon"]: 150,
        }


    # MedSea
    elif str_compare(data_set, "MedSea"):
        #directory = "/marula/emason/C3S2_520/Med_Sea/"
        directory = "/home/emason/C3S2_520/Med_Sea/"
        drop_variables = [
            "crs",
            "lat_bnds",
            "lon_bnds",
            # "nv",
            "err",
        ]
        variables = dict(
            [
                ("time", "time"),
                ("lat", "latitude"),
                ("lon", "longitude"),
                ("sla", "sla"),
                ("U", "ugosa"),
                ("V", "vgosa"),
            ]
        )
        if free_period:
            ncfiles = sorted(
                glob(directory + "dt_med_twosat_phy_l4_????????_vDT2018.nc")
            )
            chunks = {
                variables["time"]: "auto",
                variables["lat"]: 56,
                variables["lon"]: 120,
        }

    ds = xr.open_mfdataset(
        ncfiles,
        combine="nested", #specify combine='nested' along with a value for `concat_dim`.
        decode_times=True,
        concat_dim="time",
        parallel=True,
        drop_variables=drop_variables,
        chunks=chunks,
        persist=True,
        engine="netcdf4",
    )

    uv = zeros((1, *ds[variables["U"]].shape[1:]))
    uv_trend = zeros((2, *ds[variables["U"]].shape[1:]))
    lats = ds[variables["lat"]].values
    lats_i = lats.nonzero()[0]
    lats_i = hstack((lats_i[::36], lats_i.max()))
    lat0, lat1 = lats_i[:-1], lats_i[1:]

    time = date2num(ds[variables["time"]])

    u = zeros(ds[variables["U"]].shape[1:])
    v = zeros(ds[variables["V"]].shape[1:])

    for lt0, lt1 in zip(lat0, lat1):
        print(lt0, lt1, lats[lt0], lats[lt1])
        lat_slc = slice(lats[lt0], lats[lt1])
        try:
            ds_uv_slc = ds.sel(latitude=lat_slc)
        except Exception:
            try:
                ds_uv_slc = ds.sel(lat=lat_slc)
            except Exception:
                ds_uv_slc = ds.sel(yu_ocean=lat_slc)

        ds_uv_slc = ds_uv_slc.chunk({variables["time"]: -1})
        u = ds_uv_slc[variables["U"]].values
        v = ds_uv_slc[variables["V"]].values

        out = empty((1, *u[0].shape), dtype=float)
        data_out_tmp = xr.apply_ufunc(
            get_uvspd_quantile,
            u,
            v,
            out,
            input_core_dims=["time", "latitude", "longitude"],
            output_core_dims=[[]],
            dask="parallelized",
        )
        uv[0, lt0 : lt1 + 1] = data_out_tmp[0]


        data_out_tmp = xr.apply_ufunc(
            get_slope,
            u,
            v,
            [time, nyear],
            input_core_dims=["time", "latitude", "longitude"],
            output_core_dims=[[]],
            dask="parallelized",
        )
        uv_trend[0, lt0 : lt1 + 1] = data_out_tmp[0]
        uv_trend[1, lt0 : lt1 + 1] = data_out_tmp[1]

        ds_uv_slc.close()

    ds["UV_extrema"] = (["latitude", "longitude"], uv[0])
    ds["U_trend"] = (["latitude", "longitude"], uv_trend[0])
    ds["V_trend"] = (["latitude", "longitude"], uv_trend[1])

    # Replaced ufunc.sqrt with math.ufunc
    ds["UV"] = numpy.sqrt(ds[variables["U"]] ** 2 + ds[variables["V"]] ** 2)
    ds["UV_mean"] = ds["UV"].mean(variables["time"])
    ds["UV_std"] = ds["UV"].std(variables["time"])

    # ds["UV_std"] = xr.ufuncs.sqrt(((ds["UV"] - ds["UV_mean"]) ** 2 ).mean('time'))
    # sasa
    # time = date2num(ds[variables["time"]])
    ds["nyear"] = nyear

    if str_compare(data_set, "Altimetry") or str_compare(data_set, "Black_Sea"):
        print("---- Coverage only applies for Altimetry data")
        uv_coverage = ds[variables["U"]].count("time")
        uv_coverage /= uv_coverage.max()
        coverage = uv_coverage != 0
        ds["Coverage"] = uv_coverage.where(coverage)  # set landmask
        ds["Coverage_MEAN"] = uv_coverage.mean().values
        ds["Coverage_STD"] = uv_coverage.std().values
        ds["UV_mean"] = ds["UV_mean"].where(coverage)
        ds["UV_std"] = ds["UV_std"].where(coverage)
        ds["U_trend_mean"] = ds["U_trend"].where(coverage).mean()
        ds["V_trend_mean"] = ds["V_trend"].where(coverage).mean()
        ds["U_trend_std"] = ds["U_trend"].where(coverage).std()
        ds["V_trend_std"] = ds["V_trend"].where(coverage).std()
        print("---- Mean")
        u_mean = ds[variables["U"]].mean("time").where(coverage).values
        v_mean = ds[variables["V"]].mean("time").where(coverage).values
        u_std = ds[variables["U"]].std("time").where(coverage).values
        v_std = ds[variables["V"]].std("time").where(coverage).values
        ds["UV_extrema"] = ds["UV_extrema"].where(coverage)

    ds["U_mean"] = (["latitude", "longitude"], u_mean)
    ds["V_mean"] = (["latitude", "longitude"], v_mean)
    ds["U_std"] = (["latitude", "longitude"], u_std)
    ds["V_std"] = (["latitude", "longitude"], v_std)
    ds["UV_extrema_mean"] = nanmean(uv[0])
    ds["UV_extrema_std"] = nanstd(uv[0])

    ds["U_mean_time_series"] = ds[variables["U"]].mean(
        [variables["lon"], variables["lat"]]
    )
    ds["V_mean_time_series"] = ds[variables["V"]].mean(
        [variables["lon"], variables["lat"]]
    )

    ds = ds.drop_vars([variables["U"], variables["V"], "UV"])
    ds = ds.load()

    if free_period:
        ds.to_netcdf(savedir + "C3S_currents_from_%s.nc" % data_set)
    else:
        ds.to_netcdf(savedir + "C3S_currents_from_%s_1993_2021.nc" % data_set)
